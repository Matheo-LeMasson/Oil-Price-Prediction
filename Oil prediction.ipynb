{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e59a842-e2e0-46d2-abd8-df5760f757f8",
   "metadata": {},
   "source": [
    "# Oil prediction analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93b2f94-2d38-40b4-91f7-34d900ccbdf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ba82b801e32b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRFE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import operator as op\n",
    "import typing as t\n",
    "import statistics\n",
    "import shap\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import abc\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.auto import trange\n",
    "from tqdm import trange\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from eBoruta import eBoruta, TrialData, Features, Dataset, setup_logger  \n",
    "# pip install git+https://github.com/edikedik/eBoruta.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39430e0-c7e1-4608-89f7-c8507df994bb",
   "metadata": {},
   "source": [
    "# Load and analysis of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77dc662-863c-49d8-a0f3-5c6a8c9fdd3e",
   "metadata": {},
   "source": [
    "We selected some parameters that could influence the price of oil such as:  \n",
    "'Year', 'Price Cononut oil', 'Price Sugar', 'Us crude oil reserves', 'Quantity oil embarked', 'Quantity goods embarked', 'Pandemic (covid)', 'War','Electric car registrations', 'World imports', 'World exports', 'Value of Solar Consumption', 'Inflation', 'Value of Wind Consumption', 'Value of Nuclear Consumption', 'Value of Natural Gas Consumption', 'Population', 'OPEC cuts on production', 'Price gold', 'GDP Growth','Crude oil and NGL production', 'World-oil demand', 'Value of Freight Transport'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42824a49-cc00-4ea2-91da-1c53ea998afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv\n",
    "df = pd.read_csv('data_final1.csv')\n",
    "\n",
    "x = df.drop(['Oil brent price ($/bbl)'], 1)\n",
    "y = df['Oil brent price ($/bbl)']\n",
    "\n",
    "# Normalize x\n",
    "min_vals = x.min()\n",
    "max_vals = x.max()\n",
    "x = (x - min_vals) / (max_vals - min_vals)\n",
    "\n",
    "display(df.describe())\n",
    "\n",
    "corr_matrix = df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "df.hist(figsize=(10, 8), bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['Year'], df['Oil brent price ($/bbl)'])\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Oil brent price ($/bbl)')\n",
    "plt.title('Oil brent price ($/bbl) over time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c3820-712e-460f-9742-902c56709e03",
   "metadata": {},
   "source": [
    "# Explore different models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff1e9c-9873-4a12-816a-af2d71685c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8704d7-d95e-4a45-a783-bde923968d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d0b4d-521b-46c0-b698-d35d6cc6113d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0ab37-7a1c-4d2b-b0eb-5d5db1ea20d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668387b-c60a-4b90-a492-fea103904f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019a2b53-86c0-4828-97bd-0cbf757478b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model, x, y, n, score_fn, agg_fn, stratified):\n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits=n)\n",
    "    else:\n",
    "        kf = KFold(n_splits=n)\n",
    "\n",
    "    scores = []\n",
    "    for train_idx, test_idx in kf.split(x, y):\n",
    "        X_train, Y_train = x.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_test, Y_test = x.iloc[test_idx], y.iloc[test_idx]\n",
    "        model.fit(X_train, Y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = score_fn(Y_test, y_pred)\n",
    "        scores.append(score)\n",
    "    return agg_fn(scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1e0fc-4639-48e2-952b-4c9470f5d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.columns)\n",
    "df = pd.read_csv('data_final1.csv')\n",
    "x = df.loc[:, df.columns != 'Oil brent price ($/bbl)']\n",
    "y = df['Oil brent price ($/bbl)']\n",
    "selected_columns = ['Year', 'World imports', 'Price gold', 'War', 'Inflation']\n",
    "# selected_columns2 = ['Year', 'World imports', 'Cononut oil ($/mt)', 'Sugar ($/kg)', 'Inflation']\n",
    "xs = x.loc[:, selected_columns]\n",
    "\n",
    "# define models to cross-validate\n",
    "models = []\n",
    "models.append(('XGBRegressor',XGBRegressor(),mean_squared_error,np.mean, False))\n",
    "#models.append(('DecisionTree', DecisionTreeRegressor(),mean_squared_error,np.mean, False)) \n",
    "models.append(('RandomForest', RandomForestRegressor(),mean_squared_error,np.mean, False))\n",
    "#models.append(('LogisticRegression', LogisticRegression(),mean_squared_error,np.mean, False))\n",
    "stratified=0\n",
    "\n",
    "# evaluate each model using cross-validation\n",
    "results = []\n",
    "n=4\n",
    "for name, model, score_fn, agg_fn, stratified in models:\n",
    "    print (name, model, score_fn, agg_fn, stratified )\n",
    "    score=cross_validate(model, x, y, n, score_fn, agg_fn, stratified)\n",
    "    score1=cross_validate(model, xs, y, n, score_fn, agg_fn, stratified)\n",
    "    # results.append(score)\n",
    "    print(f'x: {score}')\n",
    "    print(f'xs: {score1}')\n",
    "\n",
    "# best_model = models[np.argmin(results)]\n",
    "# print(f'Best_model={best_model}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcb089-a3ed-46be-865a-65e5af39e525",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad2203a6",
   "metadata": {},
   "source": [
    "## PCA transformation on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import and normalize the data\n",
    "\n",
    "df = pd.read_csv('data_final1.csv')\n",
    "x = df.loc[:, df.columns != 'Oil brent price ($/bbl)']\n",
    "y = df['Oil brent price ($/bbl)']\n",
    "\n",
    "x = (x - x.min()) / (x.max() - x.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "print(explained_variance_ratio)\n",
    "print(len(explained_variance_ratio))\n",
    "\n",
    "# Determine the number of components that explain 90% of the variance\n",
    "n_components = np.argmax(explained_variance_ratio >= 0.9) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "selected_features = pca.components_\n",
    "print(len(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570b079",
   "metadata": {},
   "source": [
    "We need 5 features to get 90% of the variance explained, just as many features as our selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, 10), explained_variance_ratio[:9])\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Ratio of explained variance')\n",
    "plt.axhline(y=0.9, color='red', linestyle='--')\n",
    "plt.axvline(x=5, color='yellow', linestyle='--')\n",
    "plt.axvline(x=4, color='green', linestyle='--')\n",
    "plt.title('Ratio of explained variance over number of components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=8)\n",
    "pca.fit(x)\n",
    "X_transformed = pca.transform(x)\n",
    "\n",
    "plt.plot(range(1, 9), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Ratio of explained variance by new component')\n",
    "plt.axhline(y=0.025, color='red', linestyle='--')\n",
    "plt.axvline(x=5, color='yellow', linestyle='--')\n",
    "plt.axvline(x=6, color='green', linestyle='--')\n",
    "plt.title('Ratio of explained variance by new component over the number of components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a7691",
   "metadata": {},
   "source": [
    "After 5 components, less than 2.5% of the variance is explained by adding a new parameter, another reason to keep 5 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70001937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to cross-validate\n",
    "models = []\n",
    "models.append(('XGBRegressor',XGBRegressor(),mean_squared_error,np.mean, False))\n",
    "models.append(('DecisionTree', DecisionTreeRegressor(),mean_squared_error,np.mean, False)) \n",
    "models.append(('RandomForest', RandomForestRegressor(),mean_squared_error,np.mean, False))\n",
    "\n",
    "models_PCA = []\n",
    "models_PCA.append(('XGBRegressor',XGBRegressor(),mean_squared_error,np.mean, False))\n",
    "models_PCA.append(('DecisionTree', DecisionTreeRegressor(),mean_squared_error,np.mean, False)) \n",
    "models_PCA.append(('RandomForest', RandomForestRegressor(),mean_squared_error,np.mean, False))\n",
    "#models.append(('LogisticRegression', LogisticRegression(),mean_squared_error,np.mean, False))\n",
    "stratified=0\n",
    "\n",
    "# evaluate each model using cross-validation\n",
    "results = []\n",
    "n=4\n",
    "for name, model, score_fn, agg_fn, stratified in models:\n",
    "    print('raw data')\n",
    "    print (name, model, score_fn, agg_fn, stratified )\n",
    "    score=cross_validate(model, x, y, n, score_fn, agg_fn, stratified)\n",
    "    results.append(score)\n",
    "    print(score)\n",
    "    \n",
    "results_PCA = []\n",
    "n=4\n",
    "for name, model, score_fn, agg_fn, stratified in models_PCA:\n",
    "    print('data transformed by PCA')\n",
    "    print (name, model, score_fn, agg_fn, stratified )\n",
    "    score=cross_validate(model, x_pca, y, n, score_fn, agg_fn, stratified)\n",
    "    results_PCA.append(score)\n",
    "    print(score)\n",
    "\n",
    "best_model = models[np.argmin(results)]\n",
    "print(f'Best_model={best_model}')\n",
    "best_model_PCA = models_PCA[np.argmin(results_PCA)]\n",
    "print(f'Best_model={best_model_PCA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a23a8e",
   "metadata": {},
   "source": [
    "First, notice that the model with the lowest mean squared error is XGB regressor in both cases.\n",
    "\n",
    "We also see that using PCA with 5 components on our data is very efficient as it reduces our error from 621 to 253, more than divides it by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255918ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f52de9c-7ab8-4675-bc0a-5c95fb6e6bb8",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da8aff-633c-4118-bf04-b2b609c2dce3",
   "metadata": {},
   "source": [
    "## Wrapper method  \n",
    "##   -  Sequential Feature Selector \"forward\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a847e-9186-4765-89f2-88ae4653013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [RandomForestRegressor, XGBRegressor]\n",
    "\n",
    "for estimator in estimators: \n",
    "    selector = SequentialFeatureSelector(estimator=estimator(),\n",
    "                                         n_features_to_select=5,\n",
    "                                         direction='forward')\n",
    "\n",
    "    selector.fit(x, y)\n",
    "\n",
    "\n",
    "    selected_features = x.columns[selector.support_].tolist()\n",
    "    \n",
    "\n",
    "    print(selected_features)\n",
    "    \n",
    "    estimator = estimator()\n",
    "    estimator.fit(x[selected_features], y)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(estimator)\n",
    "    shap_values = explainer.shap_values(x[selected_features])\n",
    "    \n",
    "    shap.summary_plot(shap_values, x[selected_features], feature_names=selected_features)\n",
    "    \n",
    "    selected_feature_importances = pd.DataFrame({'Feature': selected_features, 'Importance': np.abs(shap_values).mean(axis=0)})\n",
    "    selected_feature_importances = selected_feature_importances.sort_values(by='Importance', ascending=False)\n",
    "    top_5_features = selected_feature_importances['Feature'].head(5).tolist()\n",
    "        \n",
    "        \n",
    "    print(f\"Estimator: {estimator}\")\n",
    "    print(f\"selected_features: {top_5_features}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25922c8c-468a-4ebc-b616-cdf896ade350",
   "metadata": {},
   "source": [
    "## -  Recursive Feature Elimination "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e122c-cef0-47da-bd96-8ceab01bdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [RandomForestRegressor, XGBRegressor]\n",
    "\n",
    "for estimator in estimators: \n",
    "    model = estimator()\n",
    "    rfe = RFE(estimator=model, n_features_to_select=5)\n",
    "    rfe.fit(x, y)\n",
    "    feature_indices = rfe.get_support(indices=True)\n",
    "    selected_features = x.columns[feature_indices]\n",
    "\n",
    "    selected_x = x[selected_features]\n",
    "    model.fit(selected_x, y)\n",
    "    \n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(selected_x)\n",
    "    shap.summary_plot(shap_values, selected_x, feature_names=selected_features)\n",
    "\n",
    "    selected_feature_importances = pd.DataFrame({'Feature': selected_features, 'Importance': np.abs(shap_values).mean(axis=0)})\n",
    "    selected_feature_importances = selected_feature_importances.sort_values(by='Importance', ascending=False)\n",
    "    top_5_features = selected_feature_importances['Feature'].head(5).tolist()\n",
    "        \n",
    "        \n",
    "    print(f\"Estimator: {estimator}\")\n",
    "    print(f\"selected_features: {top_5_features}\")\n",
    "    \n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6ca74-cb3d-494a-8373-a0135477fd4a",
   "metadata": {},
   "source": [
    "## - Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053eff1-12ce-4d3a-9eba-8ef1bcfe77ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_imp_history(df_history: pd.DataFrame):\n",
    "    sns.lineplot(x='Step', y='Importance', hue='Feature', data=df_history)\n",
    "    sns.lineplot(x='Step', y='Threshold', data=df_history, linestyle='--', linewidth=4)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "boruta = eBoruta(n_iter = 20, verbose=2, classification = False, test_stratify = False, pvalue = 0.001, percentile = 100) \n",
    "boruta.fit(x, y, model = XGBRegressor());\n",
    "features = boruta.features_\n",
    "print(f'features accepted: {features.accepted}')\n",
    "print(f'features rejected: {features.rejected}')\n",
    "print(f'features tentative: {features.tentative}')\n",
    "\n",
    "df = features.history\n",
    "plot_imp_history(df)\n",
    "r=boruta.rank(sort = True)\n",
    "print(r)\n",
    "dictionary = dict(zip(r['Feature'], r['Importance'].apply(lambda x: [x])))\n",
    "print(dictionary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ab1559-3028-48a8-aecf-bfd4fb39a8cd",
   "metadata": {},
   "source": [
    "## Embelled method\n",
    "## - XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a65dc2-705a-4cb9-a37a-59e46e675b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "boost = XGBRegressor()\n",
    "boost.fit(x, y)\n",
    "p_boost = boost.predict(x)\n",
    "\n",
    "feature_importance = boost.feature_importances_\n",
    "\n",
    "top_5_indices = feature_importance.argsort()[-5:][::-1]\n",
    "top_5_features = x.columns[top_5_indices]\n",
    "print(top_5_features)\n",
    "\n",
    "x_top_5 = x[top_5_features]\n",
    "\n",
    "boost.fit(x_top_5, y)\n",
    "\n",
    "explainer = shap.TreeExplainer(boost)\n",
    "\n",
    "shap_values = explainer.shap_values(x_top_5)\n",
    "\n",
    "shap.summary_plot(shap_values, x_top_5, feature_names=x.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d28ef-e2fd-46b6-8e68-c968d23eac50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff2a616c-6619-4917-8a10-a69af9372a32",
   "metadata": {},
   "source": [
    "## Filter method\n",
    "## - Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074b54b3-9233-48e4-a9e6-26654d330499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_final1.csv')\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "threshold = 0.70\n",
    "\n",
    "target_correlation = correlation_matrix['Oil brent price ($/bbl)']\n",
    "\n",
    "target_correlation_sorted = target_correlation.sort_values(ascending=False)\n",
    "\n",
    "selected_features = 0\n",
    "\n",
    "\n",
    "print(target_correlation_sorted[1:6])\n",
    "\n",
    "corr_with_target = df.corr()['Oil brent price ($/bbl)'].abs().sort_values(ascending=True)\n",
    "d = corr_with_target.to_dict()\n",
    "del d['Oil brent price ($/bbl)']\n",
    "data = pd.Series(d)\n",
    "plt.barh(data.index, data.values)\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.title(f'Correlation with Oil brent price ($/bbl)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5fcfb-1e70-4539-8cf7-4b9045c91303",
   "metadata": {},
   "source": [
    "## - ANOVA F-v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9538cbd-2a47-4f35-882d-c7178a7bfd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f_scores, p_values = f_classif(x, y)\n",
    "\n",
    "feature_scores = pd.DataFrame({'Feature': x.columns, 'F-Score': f_scores, 'p-value': p_values})\n",
    "\n",
    "feature_scores = feature_scores.sort_values('F-Score', ascending=False)\n",
    "\n",
    "print(feature_scores[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d98ab2-38a3-452f-9fc8-6ecf693e652f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9449f3c1-8674-4285-9fc3-85e022909c10",
   "metadata": {},
   "source": [
    "## Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1a6a93-7d9a-4938-a46e-e392b8668f46",
   "metadata": {},
   "source": [
    "According to the previous studies, the most important parameters, sort in descending order of importance, are:  \n",
    "- Year\n",
    "- World imports\n",
    "- World exports\n",
    "- Inflation\n",
    "- Price gold\n",
    "- War\n",
    "- OPEC cuts on production"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c1caa-faf1-4d75-9db8-7f60ba6efac6",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929564b-5eb8-40dc-a408-3a8029271669",
   "metadata": {},
   "source": [
    "We want to avoid having 2 highly correlated features in our small selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb102d-b957-4309-ad3d-c8f6ac148406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362c97e-3801-446b-824a-8aee8b599089",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df.copy()\n",
    "df2.drop(['Oil brent price ($/bbl)'], 1)\n",
    "corr_threshold = 0.95\n",
    "\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "highly_corr_features = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] >= corr_threshold:\n",
    "            feature_i = corr_matrix.columns[i]\n",
    "            feature_j = corr_matrix.columns[j]\n",
    "            highly_corr_features.add((feature_i, feature_j))\n",
    "\n",
    "print(\"highly_corr_features: \", highly_corr_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f30e2-0a81-4ca9-b4d9-0e08da5a25ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6744bb9c-2810-4179-97ec-2fc733a7f20f",
   "metadata": {},
   "source": [
    "Since \"World exports\" and \"World imports\" are highly correlated, we keep only one of the two (the more important one).  \n",
    "Here is the final selection (the most important parameters):  \n",
    "- Year\n",
    "- World imports\n",
    "- Inflation\n",
    "- Price gold\n",
    "- War"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f13412-d075-4709-aced-b4cccc30232a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80c0b59c-dcd4-4c3f-b3db-fb281de688f8",
   "metadata": {},
   "source": [
    "## Retraining with the selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db40078b",
   "metadata": {},
   "source": [
    "Now we want to compare the PCA components with our selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185621c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_selected = x[['Year', 'World imports', 'Price gold', 'War', 'Inflation']]\n",
    "\n",
    "x_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models to cross-validate\n",
    "models_PCA = []\n",
    "models_PCA.append(('XGBRegressor',XGBRegressor(),mean_squared_error,np.mean, False))\n",
    "models_PCA.append(('DecisionTree', DecisionTreeRegressor(),mean_squared_error,np.mean, False)) \n",
    "models_PCA.append(('RandomForest', RandomForestRegressor(),mean_squared_error,np.mean, False))\n",
    "\n",
    "models_selected = []\n",
    "models_selected.append(('XGBRegressor',XGBRegressor(),mean_squared_error,np.mean, False))\n",
    "models_selected.append(('DecisionTree', DecisionTreeRegressor(),mean_squared_error,np.mean, False)) \n",
    "models_selected.append(('RandomForest', RandomForestRegressor(),mean_squared_error,np.mean, False))\n",
    "\n",
    "# evaluate each model using cross-validation\n",
    "    \n",
    "results_PCA = []\n",
    "n=4\n",
    "for name, model, score_fn, agg_fn, stratified in models_PCA:\n",
    "    print('Data transformed by PCA')\n",
    "    print (name, model, score_fn, agg_fn, stratified )\n",
    "    score=cross_validate(model, x_pca, y, n, score_fn, agg_fn, stratified)\n",
    "    results_PCA.append(score)\n",
    "    print(score)\n",
    "    \n",
    "results_selected = []\n",
    "n=4\n",
    "for name, model, score_fn, agg_fn, stratified in models_selected:\n",
    "    print('Data with selected features')\n",
    "    print (name, model, score_fn, agg_fn, stratified )\n",
    "    score=cross_validate(model, x_selected, y, n, score_fn, agg_fn, stratified)\n",
    "    results_selected.append(score)\n",
    "    print(score)\n",
    "    \n",
    "\n",
    "best_model_PCA = models_PCA[np.argmin(results_PCA)]\n",
    "print(f'Best_model={best_model_PCA}')\n",
    "best_model_selected = models[np.argmin(results_selected)]\n",
    "print(f'Best_model={best_model_selected}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a1b74",
   "metadata": {},
   "source": [
    "Once again, XGB regressor is the best model.\n",
    "\n",
    "We notice that PCA has a better score than our selection: an error of 253 instead of an error of 372.\n",
    "\n",
    "We also notice that using PCA on our selection is almost useless as we obtain a very similar error of 358.\n",
    "\n",
    "If we only look at the scores, we deduce that PCA works better than our selection.\n",
    "\n",
    "But there is a major flaw in this reasoning: PCA does not permit interpretability.\n",
    "Indeed, the components selected by PCA do not represent any feature, they are created by PCA.\n",
    "\n",
    "We conclude then that our selection of feature seems very good as it has a rather small error of 372 versus the error of the original data of 621, and it allows interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c78182",
   "metadata": {},
   "source": [
    "We have also used this to confirm that our selection of 5 parameters is very good as it has a lower error than other selections we have tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031eceb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e49687",
   "metadata": {},
   "source": [
    "## Going further - Creating a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['Year', 'World imports', 'Price gold', 'War', 'Inflation']\n",
    "# selected_columns2 = ['Year', 'World imports', 'Cononut oil ($/mt)', 'Sugar ($/kg)', 'Inflation']\n",
    "xs = x.loc[:, selected_columns]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(23,)))\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # Dropout regularization with a rate of 0.2\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))  # Dropout regularization with a rate of 0.2\n",
    "\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15f353",
   "metadata": {},
   "source": [
    "This new model we created has a better score for our selected data than XGB regressor (274 instead of 372).\n",
    "\n",
    "However, we should keep in mind that neural networks are likely to overfit if we complexify them too much, which is why we only put 3 activation layers in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d45a435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
